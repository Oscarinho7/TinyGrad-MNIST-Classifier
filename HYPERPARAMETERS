# HYPERPARAMETERS EXPLORATION

## MLP (Multi-Layer Perceptron)

**Base:** `784 → 512 (SiLU) → 512 (SiLU) → 10`

| Exp | Batch | LR | Decay | Steps | **Accuracy** | Loss | Notes |
|-----|-------|-----|--------|-------|-------------|------|-------|
| #1  | 512   | 0.02 | 0.90 | 70   | 86.00% | 0.52 | LR too high, oscillates |
| #2  | 256   | 0.005| 0.95 | 200  | 96.29% | 0.21 | Good trade-off |
| **#3** | 256 | **0.003** | **0.95** | **250** | **96.65%** | **0.18** | **BEST** ✓ |
| #4  | 128   | 0.005| 0.95 | 300  | 95.33% | 0.28 | Batch too small |
| #5  | 256   | 0.005| 0.95 | 200  | 96.08% | 0.23 | Wider (1024→512), no gain |
| #6  | 256   | 0.005| 0.95 | 200  | 95.97% | 0.20 | Narrower (512→256) |
| #7  | 512   | 0.01 | 0.93 | 150  | 96.44% | 0.15 | Fast, low loss |
| #8  | 256   | 0.005| 0.98 | 300  | 96.34% | 0.18 | ReLU activation |

**Key Findings:**
- **Optimal LR:** 0.003-0.005 (too high → oscillation, too low → slow)
- **Batch size:** 256 sweet spot
- **Architecture:** 512→512 sufficient
- **Duration:** 200-250 steps enough
- **Activation:** SiLU > ReLU (marginally)

---

## CNN (Convolutional Neural Network)

**Base:** `Conv(1→32,k5) + Conv(32→32,k5) + MaxPool → Conv(32→64,k3) + Conv(64→64,k3) + MaxPool → FC(576→10)`

| Exp | Batch | LR | Decay | Steps | **Accuracy** | Loss | Notes |
|-----|-------|-----|--------|-------|-------------|------|-------|
| #1  | 128   | 0.003| 0.90 | 70   | 97.62% | 0.11 | Undertrained |
| #2  | 128   | 0.003| 0.90 | 250  | 98.14% | 0.16 | More steps help |
| **#3** | 128 | **0.002** | **0.95** | **300** | **98.31%** | **0.11** | **BEST (w/ BatchNorm)** ✓ |
| #4  | 64    | 0.002| 0.95 | 350  | 98.02% | 0.06 | Batch too small |
| #5  | 128   | 0.003| 0.95 | 250  | 98.15% | 0.13 | Deeper (3 blocks), no gain |
| #6  | 128   | 0.003| 0.90 | 200  | 97.86% | 0.03 | Lighter network |
| #7  | 128   | 0.003| 0.95 | 250  | 98.31% | 0.06 | ReLU = SiLU |
| **#8** | 128 | **0.003** | **0.95** | **250** | **98.36%** | **0.03** | **No BatchNorm** ✓ |

**Key Findings:**
- **Optimal LR:** 0.002-0.003 (lower than MLP)
- **Batch size:** 128 optimal
- **Architecture:** 2 conv blocks sufficient
- **BatchNorm:** Not necessary for MNIST (removing it = better!)
- **Activation:** ReLU ≈ SiLU, ReLU faster
- **Duration:** 250-300 steps for convergence

---

## Comparison

| Metric | MLP (#3) | CNN (#8) | Difference |
|--------|----------|----------|-----------|
| **Accuracy** | 96.65% | **98.36%** | +1.71% |
| Training time | 30s | 102s | CNN 3.4x slower |
| Inference | ~2-3 ms | ~8-10 ms | CNN 3-5x slower |
| Complexity | Simple | Spatial features | CNN leverages structure |

**When to use:**
- **MLP:** Real-time, limited devices, 96% acceptable
- **CNN:** High precision needed (98%+), resources available

---

## Best Models Summary

### MLP - Experiment #3 ✓
```
Batch: 256 | LR: 0.003 | Decay: 0.95 | Steps: 250
Architecture: 784 → 512 (SiLU) → 512 (SiLU) → 10
├─ Accuracy: 96.65% (+1.65% vs target 95%)
├─ Loss: 0.18
└─ Training: 30s
```

**Why it works:** Low LR (0.003) enables fine and stable convergence. Batch 256 balances gradient stability and speed. Standard architecture has sufficient capacity without overfitting.

### CNN - Experiment #8 ✓
```
Batch: 128 | LR: 0.003 | Decay: 0.95 | Steps: 250
Architecture: 2 conv blocks WITHOUT BatchNorm
├─ Accuracy: 98.36% (+0.36% vs target 98%)
├─ Loss: 0.03 (excellent)
└─ Training: 102s
```

**Why it works:** Removing BatchNorm simplifies AND improves results! LR 0.003 + batch 128 = stable feature learning. Ultra-low loss (0.03) = excellent generalization.

---

## Key Insights

1. **Learning Rate** = critical parameter (impact ±10%)
   - MLP: 0.003-0.005
   - CNN: 0.002-0.003
   
2. **Batch Size** = affects stability more than final accuracy
   - 128-256 optimal
   - Too small (64) = noisy gradients
   - Too large (512) = overfitting risk
   
3. **Architecture** = simple enough for MNIST
   - MLP: 512→512 sufficient
   - CNN: 2 conv blocks sufficient
   
4. **BatchNorm** = unnecessary on simple datasets
   - Removing it = lighter model AND better results!
   
5. **Training Duration**
   - MLP: 150-200 steps
   - CNN: 250-300 steps (3x more)

---

## Goals Achieved

- ✅ MLP: 96.65% (target ≥95%)
- ✅ CNN: 98.36% (target ≥98%)
- ✅ Solid hyperparameter insights
